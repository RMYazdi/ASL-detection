{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"real_time.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ImnDbDhZNg7-","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","import cv2                 # working with, mainly resizing, images\n","import numpy as np         # dealing with arrays\n","import os                  # dealing with directories\n","\n","\n","MODEL_NAME = 'final_asl_scratch.h5'\n","model=tf.keras.models.load_model(MODEL_NAME)\n","\n","\n","\n","IMG_SIZE = 50\n","nb_classes=28\n","\n","\n","\n","\n","model=model\n","\n","if os.path.exists('{}.meta'.format(MODEL_NAME)):\n","    model.load(MODEL_NAME)\n","    print('model loaded!')\n","\n","\n","# organize imports\n","import cv2\n","import imutils\n","import numpy as np\n","\n","from collections import Counter\n","\n","import time\n","\n","\n","\n","            #0    1    2     3    4      5    6    7        8     9    10   11   12   13   14    15    16   17   18   19   20   21   22   23   24    25    26   27  28\n","# out_label=['U', 'T', 'E', 'R', 'BKSP', 'Q', 'D', 'BLNK1', 'G', 'SPC', 'I', 'F', 'O', 'C', 'W', 'Y', 'BLNK', 'V', 'H', 'H', 'P', 'A', 'S', 'L', 'K', 'X', 'N', 'B', ]\n","              #0    1    2     3    4      5    6    7        8     9    10   11   12     13   14    15    16   17      18   19   20    21   22   23   24    25    26   27  28\n","out_label = ['A', 'B', 'C',   'D',  'E',  'F',  'G',  'H', 'I', 'J', 'K',  'L', 'M',  'N',  'O',  'P',  'Q',  'R',  'S', 'T',  'U',  'V', 'W', 'X',  'Y',  'Z', 'del',' ']\n","\n","\n","pre=[]\n","\n","s=''\n","cchar=[0,0]\n","c1=''\n","\n","# initialize weight for running average\n","aWeight = 0.5\n","\n","# get the reference to the webcam\n","camera = cv2.VideoCapture(0)\n","\n","# region of interest (ROI) coordinates\n","top, right, bottom, left = 170, 150, 425, 450\n","\n","# initialize num of frames\n","num_frames = 0\n","\n","flag=0\n","flag1=0\n","\n","a=''\n","\n","# keep looping, until interrupted\n","while(True):\n","    # get the current frame\n","    (grabbed, frame) = camera.read()\n","\n","    # resize the frame\n","    frame = imutils.resize(frame, width=700)\n","\n","    # flip the frame so that it is not the mirror view\n","    frame = cv2.flip(frame, 1)\n","\n","    # clone the frame\n","    clone = frame.copy()\n","\n","    # get the height and width of the frame\n","    (height, width) = frame.shape[:2]\n","\n","    # get the ROI\n","    roi = frame[top:bottom, right:left]\n","\n","    # convert the roi to grayscale and blur it\n","    gray = cv2.cvtColor(roi, cv2.IMREAD_COLOR)\n","    gray = cv2.GaussianBlur(gray, (7, 7), 0)\n","\n","    \n","    # cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n","    \n","    img=gray\n","    # img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    # img=cv2.imread(\"240fn.jpg\",cv2.IMREAD_GRAYSCALE)\n","    # img=cv2.cvtColor(bw_image,cv2.COLOR_BGR2GRAY)\n","    \n","#    img=cv2.imread(\"B_test.jpg\",cv2.IMREAD_COLOR)\n","\n","    \n","#    img = cv2.resize(img, (50,50), interpolation = cv2.INTER_AREA)\n","    img = cv2.resize(img, (50,50), interpolation = cv2.INTER_LINEAR )\n","\n","    test_data =img\n","\n","\n","\n","\n","\n","    orig = img\n","    data = img.reshape(-1,IMG_SIZE,IMG_SIZE,3)\n","    data = tf.convert_to_tensor(data,dtype=tf.float32)\n","\n","    #model_out = model.predict([data])[0]\n","#    model_out = model.predict([data])[0]\n","    # print(model_out)\n","    model_out = model.predict([data])[0]\n","        # print(model_out)\n","    pnb=np.argmax(model_out)\n","    print(str(np.argmax(model_out))+\" \"+str(out_label[pnb]))\n","\n","    pre.append(out_label[pnb]) \n","\n","\n","\n","#    a=a+str(out_label[pnb])\n","#\n","#\n","#    cv2.putText(clone,\n","#           '%s ' % (a),\n","#           (150, 150), cv2.FONT_HERSHEY_PLAIN,5,(0, 255, 0))\n","#\n","#            \n","    \n","    \n","    cv2.putText(clone,\n","           '%s ' % (str(out_label[pnb])),\n","           (450, 150), cv2.FONT_HERSHEY_PLAIN,5,(0, 255, 0))\n","\n","            \n","            \n","    \n","\n","\n","    # draw the segmented hand\n","    cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n","\n","    cv2.putText(clone,\n","                   '%s ' % (str(s)),\n","                   (10, 60), cv2.FONT_HERSHEY_PLAIN,3,(0, 0, 0))\n","\n","    # increment the number of frames\n","    num_frames += 1\n","    # time.sleep(.3)\n","    # display the frame with segmented hand\n","    cv2.imshow(\"Video Feed\", clone)\n","\n","    # observe the keypress by the user\n","    keypress = cv2.waitKey(1) & 0xFF\n","\n","    # if the user pressed \"q\", then stop looping\n","    if keypress == ord(\"q\"):\n","        break\n","\n","    elif keypress == 27:\n","        break\n","\n","# free up memory\n","camera.release()\n","cv2.destroyAllWindows()\n","\n","\n","\n","\n","\n","\n","#offline image test\n","img=cv2.imread(\"A1.jpg\",cv2.IMREAD_COLOR)\n","# img=cv2.cvtColor(bw_image,cv2.COLOR_BGR2GRAY)\n","img = cv2.resize(img, (50,50), interpolation = cv2.INTER_AREA)\n","\n","data = img.reshape(-1,50,50,3)\n","data = tf.convert_to_tensor(data,dtype=tf.float32)\n","\n","model_out = model.predict([data])[0]\n","pnb=np.argmax(model_out)\n","\n","print(str(np.argmax(model_out))+\" \"+str(out_label[pnb]))\n","\n","\n","\n","\n","#\n","#\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]}]}